{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Stock Analysis Summary\n",
        "\n",
        "This notebook provides a summary of volume analysis and signals for all tracked stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy matplotlib seaborn google-cloud-bigquery pandas-gbq plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"darkgrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Configuration\n",
        "PROJECT_ID = \"{PROJECT_ID}\"  # Will be replaced with actual project ID\n",
        "DATASET = \"trading_insights\"\n",
        "TRACKED_STOCKS = {TRACKED_STOCKS_JSON}  # Will be replaced with actual tracked stocks\n",
        "\n",
        "# Configuration file path (for updating tracked stocks)\n",
        "CONFIG_PATH = \"config/tracked_stocks.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Import and Watchlist Management\n",
        "\n",
        "This section allows you to import data for all tracked stocks and manage your watchlist.\n",
        "You can update the tracked stocks and generate new notebooks for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Function to import data for all tracked stocks\n",
        "def import_stock_data(stocks=None, days=365):\n",
        "    \"\"\"Import stock data from Alpha Vantage to BigQuery.\"\"\"\n",
        "    from google.cloud import bigquery\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    import time\n",
        "    from datetime import datetime, timedelta\n",
        "    \n",
        "    # Use all tracked stocks if none specified\n",
        "    if stocks is None:\n",
        "        stocks = []\n",
        "        for direction in TRACKED_STOCKS:\n",
        "            stocks.extend(TRACKED_STOCKS[direction])\n",
        "    \n",
        "    # Get Alpha Vantage API key\n",
        "    api_key = input(\"Enter your Alpha Vantage API key: \")\n",
        "    if not api_key:\n",
        "        print(\"API key is required\")\n",
        "        return False\n",
        "    \n",
        "    # Initialize BigQuery client\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "    \n",
        "    # Import data for each stock\n",
        "    for i, ticker in enumerate(stocks):\n",
        "        print(f\"Importing data for {ticker} ({i+1}/{len(stocks)})...\")\n",
        "        \n",
        "        try:\n",
        "            # Fetch daily data from Alpha Vantage\n",
        "            url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={ticker}&outputsize=full&apikey={api_key}\"\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "            \n",
        "            if 'Time Series (Daily)' not in data:\n",
        "                print(f\"Error fetching data for {ticker}: {data.get('Note', 'Unknown error')}\")\n",
        "                # Wait for API rate limit\n",
        "                time.sleep(12)  # 5 requests per minute = 12 seconds between requests\n",
        "                continue\n",
        "            \n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame.from_dict(data['Time Series (Daily)'], orient='index')\n",
        "            \n",
        "            # Map column names\n",
        "            column_mapping = {\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close',\n",
        "                '5. adjusted close': 'adjusted_close',\n",
        "                '6. volume': 'volume',\n",
        "                '7. dividend amount': 'dividend',\n",
        "                '8. split coefficient': 'split_coefficient'\n",
        "            }\n",
        "            \n",
        "            # Rename columns\n",
        "            df = df.rename(columns=column_mapping)\n",
        "            \n",
        "            # Convert index to datetime\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            \n",
        "            # Convert all columns to numeric\n",
        "            for col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            \n",
        "            # Add symbol column\n",
        "            df['symbol'] = ticker\n",
        "            \n",
        "            # Reset index to make date a column\n",
        "            df = df.reset_index()\n",
        "            df = df.rename(columns={'index': 'date'})\n",
        "            \n",
        "            # Select only the columns we need\n",
        "            columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'adjusted_close', 'symbol']\n",
        "            df = df[columns]\n",
        "            \n",
        "            # Convert date to string (YYYY-MM-DD format)\n",
        "            df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
        "            \n",
        "            # Upload to BigQuery\n",
        "            table_id = f\"{DATASET}.stock_{ticker}_prices\"\n",
        "            \n",
        "            # Use pandas_gbq to upload data\n",
        "            df.to_gbq(\n",
        "                destination_table=table_id,\n",
        "                project_id=PROJECT_ID,\n",
        "                if_exists='replace'\n",
        "            )\n",
        "            \n",
        "            print(f\"Successfully imported {len(df)} rows for {ticker}\")\n",
        "            \n",
        "            # Wait for API rate limit\n",
        "            time.sleep(12)  # 5 requests per minute = 12 seconds between requests\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error importing data for {ticker}: {str(e)}\")\n",
        "    \n",
        "    print(\"Stock data import completed\")\n",
        "    return True\n",
        "\n",
        "# Function to update tracked stocks\n",
        "def update_tracked_stocks(buy_stocks=None, short_stocks=None):\n",
        "    \"\"\"Update the tracked stocks configuration and generate new notebooks.\"\"\"\n",
        "    import json\n",
        "    import os\n",
        "    import shutil\n",
        "    from datetime import datetime\n",
        "    from google.colab import files\n",
        "    \n",
        "    # Get current tracked stocks\n",
        "    current_stocks = TRACKED_STOCKS.copy()\n",
        "    \n",
        "    # Update with new stocks if provided\n",
        "    if buy_stocks is not None:\n",
        "        current_stocks['buy'] = buy_stocks\n",
        "    \n",
        "    if short_stocks is not None:\n",
        "        current_stocks['short'] = short_stocks\n",
        "    \n",
        "    # Create a backup of the current configuration\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    backup_file = f\"tracked_stocks_backup_{timestamp}.json\"\n",
        "    \n",
        "    with open(backup_file, 'w') as f:\n",
        "        json.dump(TRACKED_STOCKS, f, indent=2)\n",
        "    \n",
        "    # Download the backup file\n",
        "    files.download(backup_file)\n",
        "    \n",
        "    # Update the configuration file\n",
        "    with open(CONFIG_PATH, 'w') as f:\n",
        "        json.dump(current_stocks, f, indent=2)\n",
        "    \n",
        "    print(f\"Updated tracked stocks configuration. Backup saved to {backup_file}\")\n",
        "    \n",
        "    # Generate new notebooks\n",
        "    generate_notebooks(current_stocks)\n",
        "    \n",
        "    return current_stocks\n",
        "\n",
        "# Function to generate notebooks\n",
        "def generate_notebooks(tracked_stocks):\n",
        "    \"\"\"Generate individual stock analysis notebooks from template.\"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    import re\n",
        "    from pathlib import Path\n",
        "    from google.colab import files\n",
        "    \n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = Path('notebooks')\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download template from GitHub if it doesn't exist locally\n",
        "    template_path = Path('stock_analysis_template.ipynb')\n",
        "    if not template_path.exists():\n",
        "        !wget -q https://raw.githubusercontent.com/DanielDeenik/Delphi/feature/volume-analysis/notebooks/stock_analysis_template.ipynb -O stock_analysis_template.ipynb\n",
        "    \n",
        "    # Load template\n",
        "    try:\n",
        "        with open(template_path, 'r', encoding='utf-8') as f:\n",
        "            template_content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading template file: {str(e)}\")\n",
        "        return False\n",
        "    \n",
        "    # Generate notebooks for each stock\n",
        "    generated_notebooks = []\n",
        "    \n",
        "    for direction, tickers in tracked_stocks.items():\n",
        "        for ticker in tickers:\n",
        "            try:\n",
        "                # Replace placeholders in template\n",
        "                notebook_content = template_content\n",
        "                notebook_content = notebook_content.replace('{TICKER}', ticker)\n",
        "                notebook_content = notebook_content.replace('{PROJECT_ID}', PROJECT_ID)\n",
        "                notebook_content = notebook_content.replace('{DIRECTION}', direction)\n",
        "                \n",
        "                # Replace other placeholders with dummy values (will be updated when notebook runs)\n",
        "                notebook_content = notebook_content.replace('{SPIKE_COUNT}', '0')\n",
        "                notebook_content = notebook_content.replace('{LATEST_Z_SCORE}', '0.0')\n",
        "                notebook_content = notebook_content.replace('{CURRENT_SIGNAL}', 'NEUTRAL')\n",
        "                \n",
        "                # Save notebook\n",
        "                output_path = output_dir / f\"{ticker}_analysis.ipynb\"\n",
        "                with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(notebook_content)\n",
        "                \n",
        "                generated_notebooks.append(str(output_path))\n",
        "                print(f\"Generated notebook for {ticker}: {output_path}\")\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"Error generating notebook for {ticker}: {str(e)}\")\n",
        "    \n",
        "    # Create a zip file with all notebooks\n",
        "    if generated_notebooks:\n",
        "        !zip -j stock_notebooks.zip {' '.join(generated_notebooks)}\n",
        "        files.download('stock_notebooks.zip')\n",
        "    \n",
        "    print(f\"Generated notebooks for {len(tracked_stocks['buy']) + len(tracked_stocks['short'])} stocks\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Trigger data import for all tracked stocks\n",
        "# Uncomment to run\n",
        "# import_stock_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Update tracked stocks\n",
        "# Example: update_tracked_stocks(buy_stocks=['AAPL', 'MSFT', 'GOOGL'], short_stocks=['BIDU', 'NIO', 'SNAP'])\n",
        "# Uncomment to run\n",
        "# update_tracked_stocks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch Latest Analysis Results\n",
        "\n",
        "Let's fetch the latest analysis results for all tracked stocks from BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Fetch latest analysis results from master summary table\n",
        "query = f\"\"\"\n",
        "WITH LatestAnalysis AS (\n",
        "  SELECT\n",
        "    ticker,\n",
        "    MAX(timestamp) AS latest_timestamp\n",
        "  FROM\n",
        "    `{PROJECT_ID}.{DATASET}.master_summary`\n",
        "  GROUP BY\n",
        "    ticker\n",
        ")\n",
        "SELECT\n",
        "  s.*\n",
        "FROM\n",
        "  `{PROJECT_ID}.{DATASET}.master_summary` s\n",
        "JOIN\n",
        "  LatestAnalysis l\n",
        "ON\n",
        "  s.ticker = l.ticker\n",
        "  AND s.timestamp = l.latest_timestamp\n",
        "ORDER BY\n",
        "  s.confidence DESC\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    summary_df = client.query(query).to_dataframe()\n",
        "    print(f\"Fetched analysis results for {len(summary_df)} stocks\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching analysis results: {str(e)}\")\n",
        "    # Create empty dataframe with expected columns\n",
        "    summary_df = pd.DataFrame(columns=[\n",
        "        'date', 'ticker', 'direction', 'signal', 'confidence',\n",
        "        'volume_score', 'is_spike', 'spike_strength', 'notes', 'notebook_url', 'timestamp'\n",
        "    ])\n",
        "\n",
        "# If no data is available, create sample data for demonstration\n",
        "if summary_df.empty:\n",
        "    print(\"No data available in BigQuery. Creating sample data for demonstration.\")\n",
        "    \n",
        "    # Create sample data\n",
        "    sample_data = []\n",
        "    \n",
        "    # Add buy stocks\n",
        "    for ticker in TRACKED_STOCKS['buy']:\n",
        "        sample_data.append({\n",
        "            'date': datetime.now().date(),\n",
        "            'ticker': ticker,\n",
        "            'direction': 'buy',\n",
        "            'signal': np.random.choice(['STRONG_BUY', 'POTENTIAL_REVERSAL', 'NEUTRAL'], p=[0.3, 0.3, 0.4]),\n",
        "            'confidence': np.random.uniform(0.5, 1.0),\n",
        "            'volume_score': np.random.uniform(1.0, 3.0),\n",
        "            'is_spike': np.random.choice([True, False], p=[0.3, 0.7]),\n",
        "            'spike_strength': np.random.uniform(1.0, 2.0),\n",
        "            'notes': f\"Sample data for {ticker}\",\n",
        "            'notebook_url': f\"https://colab.research.google.com/drive/your-notebook-id-for-{ticker}\",\n",
        "            'timestamp': datetime.now()\n",
        "        })\n",
        "    \n",
        "    # Add short stocks\n",
        "    for ticker in TRACKED_STOCKS['short']:\n",
        "        sample_data.append({\n",
        "            'date': datetime.now().date(),\n",
        "            'ticker': ticker,\n",
        "            'direction': 'short',\n",
        "            'signal': np.random.choice(['STRONG_SHORT', 'POTENTIAL_REVERSAL', 'NEUTRAL'], p=[0.3, 0.3, 0.4]),\n",
        "            'confidence': np.random.uniform(0.5, 1.0),\n",
        "            'volume_score': np.random.uniform(1.0, 3.0),\n",
        "            'is_spike': np.random.choice([True, False], p=[0.3, 0.7]),\n",
        "            'spike_strength': np.random.uniform(1.0, 2.0),\n",
        "            'notes': f\"Sample data for {ticker}\",\n",
        "            'notebook_url': f\"https://colab.research.google.com/drive/your-notebook-id-for-{ticker}\",\n",
        "            'timestamp': datetime.now()\n",
        "        })\n",
        "    \n",
        "    summary_df = pd.DataFrame(sample_data)\n",
        "\n",
        "# Display summary dataframe\n",
        "summary_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dashboard: Signal Summary\n",
        "\n",
        "Let's create a dashboard to visualize the signals for all tracked stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a function to generate a styled HTML table\n",
        "def generate_styled_table(df, title):\n",
        "    # Create a copy of the dataframe with selected columns\n",
        "    table_df = df[['ticker', 'direction', 'signal', 'confidence', 'volume_score', 'is_spike']].copy()\n",
        "    \n",
        "    # Add hyperlinks to ticker symbols\n",
        "    table_df['ticker'] = table_df.apply(\n",
        "        lambda row: f\"<a href='{df.loc[row.name, 'notebook_url']}' target='_blank'>{row['ticker']}</a>\",\n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    # Format confidence as percentage\n",
        "    table_df['confidence'] = table_df['confidence'].apply(lambda x: f\"{x:.0%}\")\n",
        "    \n",
        "    # Format volume score\n",
        "    table_df['volume_score'] = table_df['volume_score'].apply(lambda x: f\"{x:.2f}\")\n",
        "    \n",
        "    # Format is_spike as Yes/No\n",
        "    table_df['is_spike'] = table_df['is_spike'].apply(lambda x: \"Yes\" if x else \"No\")\n",
        "    \n",
        "    # Rename columns\n",
        "    table_df.columns = ['Ticker', 'Direction', 'Signal', 'Confidence', 'Volume Z-Score', 'Volume Spike']\n",
        "    \n",
        "    # Convert to HTML\n",
        "    html = f\"<h2>{title}</h2>\"\n",
        "    html += \"<style>\"\n",
        "    html += \"table { border-collapse: collapse; width: 100%; }\"\n",
        "    html += \"th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }\"\n",
        "    html += \"th { background-color: #f2f2f2; }\"\n",
        "    html += \"tr:hover { background-color: #f5f5f5; }\"\n",
        "    html += \"a { text-decoration: none; color: #0366d6; }\"\n",
        "    html += \"a:hover { text-decoration: underline; }\"\n",
        "    html += \"</style>\"\n",
        "    html += table_df.to_html(escape=False, index=False)\n",
        "    \n",
        "    return html\n",
        "\n",
        "# Split dataframe by direction\n",
        "buy_df = summary_df[summary_df['direction'] == 'buy'].sort_values('confidence', ascending=False)\n",
        "short_df = summary_df[summary_df['direction'] == 'short'].sort_values('confidence', ascending=False)\n",
        "\n",
        "# Generate styled tables\n",
        "buy_table = generate_styled_table(buy_df, \"Buy Candidates\")\n",
        "short_table = generate_styled_table(short_df, \"Short Candidates\")\n",
        "\n",
        "# Display tables\n",
        "display(HTML(buy_table))\n",
        "display(HTML(short_table))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volume Analysis Visualization\n",
        "\n",
        "Let's visualize the volume analysis results for all tracked stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a scatter plot of volume score vs. confidence\n",
        "fig = px.scatter(\n",
        "    summary_df,\n",
        "    x=\"volume_score\",\n",
        "    y=\"confidence\",\n",
        "    color=\"direction\",\n",
        "    size=\"spike_strength\",\n",
        "    hover_name=\"ticker\",\n",
        "    hover_data=[\"signal\", \"notes\"],\n",
        "    title=\"Volume Score vs. Confidence\",\n",
        "    labels={\n",
        "        \"volume_score\": \"Volume Z-Score\",\n",
        "        \"confidence\": \"Signal Confidence\",\n",
        "        \"direction\": \"Direction\",\n",
        "        \"spike_strength\": \"Spike Strength\"\n",
        "    },\n",
        "    color_discrete_map={\"buy\": \"green\", \"short\": \"red\"}\n",
        ")\n",
        "\n",
        "# Add ticker labels\n",
        "fig.update_traces(textposition='top center')\n",
        "fig.update_layout(\n",
        "    height=600,\n",
        "    width=900,\n",
        "    xaxis=dict(title=\"Volume Z-Score\", range=[0, max(summary_df['volume_score']) * 1.1]),\n",
        "    yaxis=dict(title=\"Signal Confidence\", range=[0, 1.1])\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Signal Distribution\n",
        "\n",
        "Let's visualize the distribution of signals across all tracked stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Count signals by type and direction\n",
        "signal_counts = summary_df.groupby(['direction', 'signal']).size().reset_index(name='count')\n",
        "\n",
        "# Create a grouped bar chart\n",
        "fig = px.bar(\n",
        "    signal_counts,\n",
        "    x=\"signal\",\n",
        "    y=\"count\",\n",
        "    color=\"direction\",\n",
        "    title=\"Signal Distribution\",\n",
        "    labels={\n",
        "        \"signal\": \"Signal Type\",\n",
        "        \"count\": \"Count\",\n",
        "        \"direction\": \"Direction\"\n",
        "    },\n",
        "    color_discrete_map={\"buy\": \"green\", \"short\": \"red\"}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    height=500,\n",
        "    width=800,\n",
        "    xaxis=dict(title=\"Signal Type\"),\n",
        "    yaxis=dict(title=\"Count\")\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top Signals\n",
        "\n",
        "Let's identify the top signals for both buy and short directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Function to display top signals\n",
        "def display_top_signals(df, direction, n=3):\n",
        "    # Filter by direction and non-neutral signals\n",
        "    filtered_df = df[(df['direction'] == direction) & (df['signal'] != 'NEUTRAL')]\n",
        "    \n",
        "    # Sort by confidence\n",
        "    top_df = filtered_df.sort_values('confidence', ascending=False).head(n)\n",
        "    \n",
        "    if len(top_df) == 0:\n",
        "        print(f\"No {direction} signals found\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Top {n} {direction.capitalize()} Signals:\")\n",
        "    for i, (_, row) in enumerate(top_df.iterrows(), 1):\n",
        "        print(f\"{i}. {row['ticker']} - {row['signal']} (Confidence: {row['confidence']:.0%})\")\n",
        "        print(f\"   Volume Z-Score: {row['volume_score']:.2f}\")\n",
        "        print(f\"   Notes: {row['notes']}\")\n",
        "        print(f\"   Notebook: {row['notebook_url']}\")\n",
        "        print()\n",
        "\n",
        "# Display top buy signals\n",
        "display_top_signals(summary_df, 'buy')\n",
        "\n",
        "# Display top short signals\n",
        "display_top_signals(summary_df, 'short')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "This notebook provides a summary of volume analysis and signals for all tracked stocks.\n",
        "\n",
        "### Key Findings:\n",
        "- Number of buy signals: {BUY_SIGNAL_COUNT}\n",
        "- Number of short signals: {SHORT_SIGNAL_COUNT}\n",
        "- Top buy candidate: {TOP_BUY_TICKER}\n",
        "- Top short candidate: {TOP_SHORT_TICKER}\n",
        "\n",
        "### Next Steps:\n",
        "1. Review individual notebooks for stocks with strong signals\n",
        "2. Monitor volume patterns for potential trade opportunities\n",
        "3. Update analysis daily to track changes in volume patterns"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Master Stock Analysis Summary",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
